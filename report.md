
Для полноты картины сначала хотелось бы сделать небольшой экскурс в историю и рассказать как все было устроено до внедрения бинарного протокола. Думаю, что глубоко и детально не будем уходить в эти дебри, а просто пробежимся по наиболее интересным моментам.

### Раздел №1
Не для кого не секрет, что в младенчестве платформа A/B тестирования позаимствовала подходы к отдаче конфигов, которые были в ходу в Яндексе (на тот момент как раз CTO был из Яндекса (Анатоликс)). А именно тот момент, что все json-конфиги передавались в хедерах. Почему так? На самом деле тут ряд моментов, и если отбросить авторитетность лиц, которые тем или иным образом принимали участие в разработки платформы, то наиболее весомый заключается в том, что такой подход позволяет минимизировать разработку со стороны пользователей платформы A/B тестирования (т.е. никакого дополнительно кода, практически не нужно было писать, а тем более поддерживать какой-то контракт), а следовательно быстрее начать проводить тесты, которые позволят доказать или опровергнуть ту или иную гипотезу.


#### Слайд 4
В результате чего все конфиги экспериментов по тому или иному сервису передавались в хедерах и это выглядело приблизительно так. И это работало до поры до времени. Но в одно прекрасное солнечное утро или день, или уже вечер. Олдскулы больше расскажут вам подробности этого замечательно инцидента...


#### Слайд 5
... НО в этот момент прилег стейдж. И долго не могли понять что же произошло. Оказалось, что в качестве злодея выступала платформа АБ тестов. Все-таки у такого подхода был недостаток. И он заключался в том, что когда количество экспериментов стало расти. Расти стал и хедер в котором как раз и фигурировали конфиги всех сервисов. И размерность хедеров стала превышать 4kb (по умолчанию это размер проставляется в более или менее нормальных балансировщиках, например, nginx). Поэтому каждый запрос возвращал ошибку.

Если честно, мне сейчас даже страшно представить что было бы, если такая же херня произошла на проде...


#### Слайд 6
Что ж проблема ясна и ее нужно зафиксить. Какой же фикс мы делаем? Правильно, мы внедряем еще одну зависимость в виде memcached. И, кажется, он был наиболее разумным в тот момент времени

Теперь на каждый запрос пользователя платформа не отдает все конфиги в виде json-а, а генерирует красивый и компактный UUID. И по этому UUID сохраняет конфиг в memcached.

Когда бэкенд-сервису необходимо узнать, какие фичи включены для пользователя, он идёт в ab-test-api, передавая туда UUID. ab-test-api в свою очередь идёт по этому ключу в Memcached, получает JSON и из него отдаёт набор включённых фичей для сервиса.


#### Слайд 7
У такой архитектуры так же есть свои недостатки. Причем некоторые из них не позволили бы развивать платформу такими темпами как за последние 1.5 - 2 года. Когда количество экспериментов росло кратно.
Тут хочется отметить наиболее важные недостатки, как мне кажется, это лишняя зависимость от memcached в который пихается херова туча данных. И дополнительный RPS, который создают клиенты 2-го уровня.


#### Слайд 8
Итак, встречайте его величество - бинарный протокол. Именно в таком формате сейчас конфиги экспериментов долетают до конечных сервисов. Некоторые задаются вопросом, так в чем же преимущество? Мы же снова пытаемся наступить на те же самые грабли, что были ранее... И я отвечу фразой, которую любит говорить Андрей: Да, но нет. Обо всем поп порядку.

Здесь тоже хотелось бы сделать небольшое лирическое отсупление. Бинарный проткол - это НЕ протокол в классическом его понимание. Это всего навсего дополнительный хедер в HTTP запросе.

#### Слайд 9
Да, на всякий случай. Кто более детально хочет погрузится в историю развития платформы A/B тестирования в Озон можно прочесть статью, которую написал наш любимый руководитель - Евгений Пак. Она на habr опубликована.

  


### Раздел №2
Вот мы и подошли к теме, которая нас всех так волнует.
Итак давайте вместе разбираться что это за зверь такой - бинарный протокол.


#### Слайд 12
Но начнем мы немного со стороны. Для начала давайте разберем что же значит клиент 2-го уровня?
Под сервисом 2-го мы понимаем сервис в который composer-api идет не напрямую через свою идеологию резолв параметров, а через N запросов. Т.е. если совсем упростить, то это сервисы в которые идут сервисы, находящиеся на 1-ом уровне от composer-api. Давайте рассмотрим на примере.
На небезызвестной борде ServiceOverview есть панель, которая называется - граф зависимостей. В ней выбираем сервис, который хотим проанализировать, в данном случае это fast-delivery-cache. Выставляем максимальную глубину запросов Dephth ingress = 7 и смотрим на каком уровне от composer-api находится интересующий сервис. На представленном слайде выделен путь, где этот сервис находится как раз на 2-ом уровне, т.к. на первом находится catalog-api.


#### Слайд 13
Прежде чем мы приступим к разбору структуры бинарного протокола, хочется отметить, что сама цель бинарного протокола в том, чтобы как можно в более сжатом виде представить информацию об экспериментах, в которые попал пользователь.

Кроме того он призван решить те проблемы которые были озвучены выше в архитектуре с memcached.
- Убираем дополнительную зависимость
- Благодаря особенностям формирования ключа мы получаем функционал, который позволит нам кэшировать конфиги сервисов непосредственно на их стороне (на стороне сервиса, который интегрирован с платформой)
- За счет кэша мы снижаем RPS, который генерируют клиенты 2-го уровня


#### Слайд 14 - 16
Теперь давайте посмотрим, а что же собой представляет хедер, который сервис ab-test-api отдает на любой запрос composer-api. Итак, структура его такова


xxxx

Тут важная часть ключа - это ревизия. Именно она позволяет поддерживать кеш на клиенте всегда в актуальном состоянии.

Для тех кто желает упороться я специально нашел коммит где реализовывался код декодинга бинарного ключа. Который впоследствии был заменен на вызов сторонней либки.


#### Слайд 17
Давайте попробуем разобраться как происходит формирование бинарного ключа.

Тут схематически показано что происходит, когда приходит composer-api и дергает один из наших резолверов.

Итак все начинается от etcd. Именно в ней хранится битовая карта, которая необходимо для того чтобы вся это магия произошла. Это карта хранится во ключу ab/tests/service/<имя сервиса>. И эта информация синхронизируется с внутренним репозиторием сервиса ab-test-api.
Далее, приходит запрос от composer-api и запускается механизм обработки этого запроса. Тут давайте не будем погружаться как ab-test-api понимает что пользователь попал в такие-то варианты экспериментов. Кажется, что это вопрос немного выходит за рамки этого доклада.
Итак запускается обработчик запроса, который в том числе возвращает ид вариантов. А дальше начинается магия. Ну как магия, так легкий фокус. С одной стороны у нас есть битовая карта, которая описывает на какой позиции должен находится вариант с другой стороны есть ид вариантов в которые попал пользователь. Мапим одно с др. и получаем последнюю и предпоследнюю части бинарного ключа. Первые 2 части заполняются весьма просто.

Хочется отметить, что кодирование осуществляется по каждому сервису отдельно.

Да, важный момент, который может быть нигде не описан кроме комментариев в коде. Это то что порядок сервисов в бинарном ключе строго регламентирован. Что это значит? Это значит что все сервисы в ключе расположены в порядке возрастания своего идентификатора. Это позволяет по большей части декодировать ключ не полностью, а только лишь малую его часть.


#### Слайд 18
Давайте теперь рассмотрим как бинарный ключ доезжает до сервисов 2-го уровня.


#### Слайд 19
Для тех кому более понятна диаграмма последовательностей вызовов вызов происходит таким образом.


#### Слайд 20
Куда же без этого слайда? Конечно же решив проблемы и исключив недостатки предыдущего подхода с memcached, бинарный протокол привнес и свои. Хотя эти пункты больше относятся к особенностям, чем к каким либо недостаткам.




### Раздел №3
И вот мы подошли к следующей критической точке нашего погружения. Как же формируется битовая карта?

Но тут не так все страшно. На самом деле когда я рисовал эту призу. Мне показалось, что эту информацию не стоило было выделять в отдельный пункт. Но тем не менее все равно это сделал, т.к. логика описанная здесь задействует др. наш сервис,а именно ab-controller-api

Итак, поехали.

#### Слайд 23
На самом деле тут все просто о чем свидетельствует представленная схема. Давайте быстренько пройдемся по ней.

Есть некий триггер, который запускает сбор всех вариантов активных экспериментов. После этого эти варианты сортируются в порядке возрастания и на против каждого проставляется порядковый номер.
Одновременно с этим инкрементиться ревизия у сервиса.

Собственно все)


#### Слайд 24
Давайте теперь посмотрим, что же это за триггер? Или какие события приводят к перестроению битовой карты. Вот они.

Тут стоить отметить п.9, т.к. про него мы все время забываем, но такое в нашей системе возможно, а следовательно битовая карта должна быть перестроена.


#### Слайд 25
На самом деле все эти пункты можно свести к одному правилу. Если существует действие, которое приводит к изменению конечного конфига, то необходимо перестроить битовую карту




### Раздел №4
В этом разделе я собрал немного статистики, чтобы наглядно показать, что бинарный протокол это реально прорывная вещь в нашей архитектуре.


#### Слайд 28
И тут хочется начать с понимания того, а сколько же сервисов в нашей админке используют бинарный протокол.

Как видите их не много, а каких-то 27%


#### Слайд 29
Но давайте взглянем на это слайд. Это количество экспериментов, которые были запущены на эти сервисы. Т.е. каких-то там 27% сервисов 2-го уровня генерят в нашей платформе более половины экспериментов. Как говорится - "Как тебе такое Илон Маск?"


#### Слайд 30
Ну ок. Давайте теперь посмотрим какой RPS создают сервисы 2-го уровня.

Тут хочется отметить, что RPS от сервисов 2-го уровня это не совсем реальный RPS, который пришелся на сервис ab-test-api, а скорее под описание этого RPS подходит термин "предотвращенный RPS"


#### Слайд 31
Ну и давайте взглянем на наших рекордсменов.

Тут для некоторых, наверняка, неожиданность почему в списке нет product-facade. Но это действительно так... Этот сервис оказался в 10, но не в 5
